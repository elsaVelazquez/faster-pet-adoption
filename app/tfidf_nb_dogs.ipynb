{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "' heavily borrowed from https://github.com/GalvanizeDataScience/lectures/blob/Denver/text-classification/frank-burkholder/naive_bayes_sklearn.ipynb'"
     },
     "metadata": {},
     "execution_count": 92
    }
   ],
   "source": [
    "''' heavily borrowed from https://github.com/GalvanizeDataScience/lectures/blob/Denver/text-classification/frank-burkholder/naive_bayes_sklearn.ipynb'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['adoptable', 'adopted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(722, 18)\n0      Meet Ruff, Ruff is a smaller breed - probably ...\n1      ______ DOB/AGE: 07/10/2019 WEIGHT (GROWN):  55...\n2      Okay people, prepare yourself for this face. M...\n3      Louie is 15 months old and 42 lbs. He is a Ger...\n4      My name is Kenny. I&#039;m a 1 yr old, male bo...\n                             ...                        \n717    Dina is only about 1 1/2 yrs old.   Adoption f...\n718    Meet Virgil! Virgil is a happy happy dog! He i...\n719    Meet Omar, Omar will be a small/medium sized d...\n720    DORA  Female SHEP MIX BRINDLE 13 2DHPP KC 2/15...\n721    Meet Nova!  Now that she is done raising her p...\nName: description, Length: 722, dtype: object\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,\n       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n       1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,\n       1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,\n       0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n       0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n       1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,\n       1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n       1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,\n       0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n       1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,\n       1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n       1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,\n       0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n       1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n       1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n       0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,\n       1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n       1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n       1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,\n       0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n       0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n       0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,\n       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n       1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n       1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n       1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n       1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1], dtype=uint8)"
     },
     "metadata": {},
     "execution_count": 128
    }
   ],
   "source": [
    "path_csv = '../data/csv/giant_valid_csv.csv'\n",
    "df1 = pd.read_csv(path_csv)\n",
    "# df = df.dropna()\n",
    "# print(df.columns)\n",
    "df = df1.fillna(\"None\")\n",
    "df.drop(['photos', 'videos', 'distance', 'status_changed_at', 'published_at', 'distance', 'contact', 'organization_animal_id', 'type', 'photos'], axis = 1, inplace= True)\n",
    "\n",
    "df = pd.get_dummies(df, columns=['status'])\n",
    "df.drop(\"status_adoptable\", axis = 1, inplace=True)\n",
    "print(df.shape)\n",
    "# new = old[['A', 'C', 'D']].copy()\n",
    "\n",
    "df_content = df[[\"status_adopted\", \"description\"]].copy()\n",
    "# print(df_content.head())\n",
    "\n",
    "X = df_content[\"description\"] #data\n",
    "X.to_numpy()\n",
    "print(X)\n",
    "\n",
    "y = df_content[\"status_adopted\"] #target\n",
    "y.to_numpy()\n",
    "# print(y)  0 == negative,  1 == positive                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Meet Ruff, Ruff is a smaller breed - probably around 10 lbs at  6-7  months old and a complete mix...\n"
    }
   ],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eighty_percent_of_data = int(len(X) * .8)\n",
    "# twenty_percent_of_data = int(len(X) * .2)\n",
    "\n",
    "# X_train = X[:eighty_percent_of_data]\n",
    "# y_train = y[:eighty_percent_of_data]\n",
    "# X_test = X[:twenty_percent_of_data]\n",
    "# y_test = y[:eighty_percent_of_data]\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "722\n722\n"
    }
   ],
   "source": [
    "# print(df_content.head())\n",
    "# print(X)\n",
    "# print(y)\n",
    "print(len(X)) #577\n",
    "print(len(y)) #577"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO make my own stopwords list\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "new_stopwords = ['old', 'mix', 'dogs', '039', 'amp', 'sweet', 'year', 'years', 'website' , 'loves', 'adoption', 'application', 'shelter', 'rescue', 'rescued']\n",
    "new_stopwords_list = stop_words.union(new_stopwords)\n",
    "\n",
    "count_vect = CountVectorizer(lowercase=True, tokenizer=None, stop_words = new_stopwords_list,  analyzer='word', max_df=1.0, min_df=1,  max_features=None) \n",
    "                             \n",
    "\n",
    "count_vect.fit(X)\n",
    "\n",
    "target_names = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n                ngram_range=(1, 1), preprocessor=None,\n                stop_words={'039', 'a', 'about', 'above', 'adoption', 'after',\n                            'again', 'against', 'ain', 'all', 'am', 'amp', 'an',\n                            'and', 'any', 'application', 'are', 'aren',\n                            \"aren't\", 'as', 'at', 'be', 'because', 'been',\n                            'before', 'being', 'below', 'between', 'both',\n                            'but', ...},\n                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                tokenizer=None, vocabulary=None)\n"
    }
   ],
   "source": [
    "print(count_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The type of X_counts is <class 'scipy.sparse.csr.csr_matrix'>.\nThe X matrix has 722 rows (documents) and 1806 columns (words).\n"
    }
   ],
   "source": [
    "\n",
    "X_counts = count_vect.transform(X)\n",
    "print(\"The type of X_counts is {0}.\".format(type(X_counts)))\n",
    "print(\"The X matrix has {0} rows (documents) and {1} columns (words).\".format(\n",
    "        X_counts.shape[0], X_counts.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer(use_idf=True)\n",
    "tfidf_transformer.fit(X_counts)\n",
    "X_tfidf = tfidf_transformer.transform(X_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y.values, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model = MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n",
    "nb_model.fit(X_train, y_train);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "   # save_documents = open(\"pickled_algos/all_pos_words.pickle\",\"wb\")\n",
    "    # pickle.dump(positive_cleaned_tokens_list, save_documents)\n",
    "    # save_documents.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nTarget: 0, name: 1\nTop 20 tokens:  ['none', 'good', 'dog', 'girl', 'meet', 'female', 'pit', 'foster', 'name', 'pounds', 'please', 'friendly', 'cats', 'boy', 'around', 'weight', 'fill', 'breed', 'spayed', 'playful']\n\nTarget: 1, name: 1\nTop 20 tokens:  ['online', 'none', 'fill', 'official', 'meet', 'home', 'looking', 'dog', 'forever', 'puppy', 'name', 'lbs', 'male', 'please', 'hi', '20', 'came', 'months', 'shepherd', 'month']\n"
    }
   ],
   "source": [
    "feature_words = count_vect.get_feature_names()\n",
    "n = 20 #number of top words associated with the category that we wish to see\n",
    "\n",
    "for cat in range(len(categories)):\n",
    "    print(f\"\\nTarget: {cat}, name: {target_names[cat]}\")\n",
    "    log_prob = nb_model.feature_log_prob_[cat]\n",
    "    i_topn = np.argsort(log_prob)[::-1][:n]\n",
    "    features_topn = [feature_words[i] for i in i_topn]\n",
    "    print(f\"Top {n} tokens: \", features_topn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = nb_model.predict(X_test)\n",
    "# X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.8547008547008548"
     },
     "metadata": {},
     "execution_count": 145
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test, y_predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0.74267226, 0.75533722, 0.86510355, 0.90499006, 0.82601062,\n       0.90632596, 0.68634114, 0.9805145 , 0.59918901, 0.78531774,\n       0.71571916, 0.81663639, 0.6444431 , 0.66577122, 0.77986978,\n       0.59392476, 0.72380875, 0.42476539, 0.74355175, 0.9847171 ,\n       0.87370884, 0.83443539, 0.42476539, 0.9805145 , 0.82072539,\n       0.55844652, 0.74944841, 0.88016423, 0.74412996, 0.78207221,\n       0.65531133, 0.94498941, 0.92683811, 0.42476539, 0.70674334,\n       0.63802039, 0.42476539, 0.64130663, 0.81612556, 0.7956922 ,\n       0.81659505, 0.8230588 , 0.83747452, 0.97320235, 0.42476539,\n       0.76310476, 0.9805145 , 0.83398418, 0.59579996, 0.71861768,\n       0.66375596, 0.67124824, 0.6868805 , 0.80449991, 0.92862123,\n       0.9009087 , 0.29009143, 0.69197277, 0.83177398, 0.84629935,\n       0.72103772, 0.63165531, 0.7221174 , 0.83729714, 0.66642299,\n       0.77101728, 0.7737819 , 0.63652364, 0.85494257, 0.63685885,\n       0.60805294, 0.89994946, 0.88494304, 0.60997547, 0.6873759 ,\n       0.81474295, 0.79902843, 0.21600243, 0.42476539, 0.81399222,\n       0.42476539, 0.75269283, 0.66627072, 0.42476539, 0.9465377 ,\n       0.91661634, 0.42476539, 0.89750377, 0.92934565, 0.70214864,\n       0.6983472 , 0.76408238, 0.62771954, 0.88009661, 0.73761492,\n       0.75850551, 0.91661634, 0.74470733, 0.72283725, 0.77175554,\n       0.87285746, 0.91243143, 0.70492702, 0.9805145 , 0.58781601,\n       0.95469975, 0.6606426 , 0.62823964, 0.89750377, 0.89052057,\n       0.73878003, 0.73350664, 0.59477046, 0.79462263, 0.42476539,\n       0.78186918, 0.42476539, 0.83602224, 0.57547277, 0.74913545,\n       0.94930353, 0.9419127 , 0.859332  , 0.70420724, 0.83860567,\n       0.79780854, 0.42476539, 0.68686493, 0.75314671, 0.88990237,\n       0.68277434, 0.93473397, 0.69667604, 0.78067372, 0.79462263,\n       0.79622103, 0.64250018, 0.8018665 , 0.93635792, 0.73595671,\n       0.58282052, 0.8295324 , 0.6983472 , 0.68374478, 0.82380426])"
     },
     "metadata": {},
     "execution_count": 146
    }
   ],
   "source": [
    "y_pred_proba = nb_model.predict_proba(X_test)[:,1]\n",
    "y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n       0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n       0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,\n       1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,\n       1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,\n       1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,\n       0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1])"
     },
     "metadata": {},
     "execution_count": 147
    }
   ],
   "source": [
    "thresh=0.7\n",
    "y_pred = (y_pred_proba>=thresh).astype(int)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.7783251231527093"
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "# nb_pipeline = Pipeline([('vect', CountVectorizer()),\n",
    "#                         ('tfidf', TfidfTransformer()),\n",
    "#                         ('model', MultinomialNB()),\n",
    "#                         ])\n",
    "# nb_pipeline.fit(twenty_train.data, twenty_train.target);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# twenty_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "#                                      shuffle=True, random_state=42)\n",
    "# docs_test = twenty_test.data\n",
    "# predicted = nb_pipeline.predict(docs_test)\n",
    "# accuracy = np.mean(predicted == twenty_test.target)\n",
    "# print(\"\\nThe accuracy on the test set is {0:0.3f}.\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next steps:\n",
    "# 1. pickle and save the model \n",
    "import pickle\n",
    "save_documents = open(\"pickled_algos/pickled_nb.pickle\",\"wb\")\n",
    "pickle.dump(nb_model, save_documents)\n",
    "save_documents.close()\n",
    "\n",
    "tfidf = tfidf_transformer\n",
    "save_documents = open(\"pickled_algos/tfidf_transformer.pickle\", \"wb\")\n",
    "pickle.dump(tfidf, save_documents)\n",
    "save_documents.close()\n",
    "\n",
    "count_vect = count_vect\n",
    "save_documents = open(\"pickled_algos/count_vect.pickle\", \"wb\")\n",
    "pickle.dump(count_vect, save_documents)\n",
    "save_documents.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0"
     },
     "metadata": {},
     "execution_count": 183
    }
   ],
   "source": [
    "# 2. test the model by \n",
    "#turn this whoel cell into a function that takes in a string\n",
    "#predict_one\n",
    "with open('pickled_algos/pickled_nb.pickle', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "with open('pickled_algos/tfidf_transformer.pickle', 'rb') as f:\n",
    "    tfidf = pickle.load(f)\n",
    "\n",
    "with open('pickled_algos/count_vect.pickle', 'rb') as f:\n",
    "    cv = pickle.load(f)\n",
    "\n",
    "string_pred = ['this girl is a foster pit and has none of her teeth']  \n",
    "\n",
    "cv_transformed = cv.transform(string_pred) #counts how many words\n",
    "tfidf_transformed = tfidf.transform(cv_transformed)  #tf == cv . \n",
    "string_predicted = model.predict(tfidf_transformed) \n",
    "string_predicted[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}