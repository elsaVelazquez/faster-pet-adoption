{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' heavily borrowed from https://github.com/GalvanizeDataScience/lectures/blob/Denver/text-classification/frank-burkholder/naive_bayes_sklearn.ipynb'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(722, 18)\n0      Meet Ruff, Ruff is a smaller breed - probably ...\n1      ______ DOB/AGE: 07/10/2019 WEIGHT (GROWN):  55...\n2      Okay people, prepare yourself for this face. M...\n3      Louie is 15 months old and 42 lbs. He is a Ger...\n4      My name is Kenny. I&#039;m a 1 yr old, male bo...\n                             ...                        \n717    Dina is only about 1 1/2 yrs old.   Adoption f...\n718    Meet Virgil! Virgil is a happy happy dog! He i...\n719    Meet Omar, Omar will be a small/medium sized d...\n720    DORA  Female SHEP MIX BRINDLE 13 2DHPP KC 2/15...\n721    Meet Nova!  Now that she is done raising her p...\nName: description, Length: 722, dtype: object\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,\n       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n       1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,\n       1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,\n       0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n       0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n       1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,\n       1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n       1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,\n       0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n       1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,\n       1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n       1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,\n       0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n       1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n       1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n       0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,\n       1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n       1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n       1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,\n       0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n       0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n       0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,\n       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n       1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n       1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n       1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n       1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1], dtype=uint8)"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "# twenty_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "#                                       shuffle=True, random_state=42)\n",
    "\n",
    "path_csv = '../data/csv/giant_valid_csv.csv'\n",
    "df1 = pd.read_csv(path_csv)\n",
    "# df = df.dropna()\n",
    "# print(df.columns)\n",
    "df = df1.fillna(\"None\")\n",
    "df.drop(['photos', 'videos', 'distance', 'status_changed_at', 'published_at', 'distance', 'contact', 'organization_animal_id', 'type', 'photos'], axis = 1, inplace= True)\n",
    "\n",
    "df = pd.get_dummies(df, columns=['status'])\n",
    "df.drop(\"status_adoptable\", axis = 1, inplace=True)\n",
    "print(df.shape)\n",
    "# new = old[['A', 'C', 'D']].copy()\n",
    "\n",
    "df_content = df[[\"status_adopted\", \"description\"]].copy()\n",
    "# print(df_content.head())\n",
    "\n",
    "X = df_content[\"description\"] #data\n",
    "X.to_numpy()\n",
    "print(X)\n",
    "\n",
    "y = df_content[\"status_adopted\"] #target\n",
    "y.to_numpy()\n",
    "# print(y)  0 == negative,  1 == positive                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Meet Ruff, Ruff is a smaller breed - probably around 10 lbs at  6-7  months old and a complete mix...\n"
    }
   ],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eighty_percent_of_data = int(len(X) * .8)\n",
    "# twenty_percent_of_data = int(len(X) * .2)\n",
    "\n",
    "# X_train = X[:eighty_percent_of_data]\n",
    "# y_train = y[:eighty_percent_of_data]\n",
    "# X_test = X[:twenty_percent_of_data]\n",
    "# y_test = y[:eighty_percent_of_data]\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "577\n577\n"
    }
   ],
   "source": [
    "# print(df_content.head())\n",
    "# print(X)\n",
    "# print(y)\n",
    "print(len(X_train)) #577\n",
    "print(len(y_train)) #577"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO make my own stopwords list\n",
    "count_vect = CountVectorizer(lowercase=True, tokenizer=None, stop_words='english',\n",
    "                             analyzer='word', max_df=1.0, min_df=1,\n",
    "                             max_features=None)\n",
    "\n",
    "count_vect.fit(X)\n",
    "\n",
    "target_names = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n                ngram_range=(1, 1), preprocessor=None, stop_words='english',\n                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                tokenizer=None, vocabulary=None)\n"
    }
   ],
   "source": [
    "print(count_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The type of X_counts is <class 'scipy.sparse.csr.csr_matrix'>.\nThe X matrix has 722 rows (documents) and 1753 columns (words).\n"
    }
   ],
   "source": [
    "\n",
    "X_counts = count_vect.transform(X)\n",
    "print(\"The type of X_counts is {0}.\".format(type(X_counts)))\n",
    "print(\"The X matrix has {0} rows (documents) and {1} columns (words).\".format(\n",
    "        X_counts.shape[0], X_counts.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer(use_idf=True)\n",
    "tfidf_transformer.fit(X_counts)\n",
    "X_tfidf = tfidf_transformer.transform(X_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1,\n       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n       1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,\n       1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,\n       0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n       0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n       1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,\n       1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n       1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1,\n       0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n       1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,\n       1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n       1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,\n       0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n       1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n       1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n       0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0,\n       1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n       1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n       1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,\n       0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n       0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n       0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,\n       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n       1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n       1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n       1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n       1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1], dtype=uint8)"
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y.values, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model = MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n",
    "nb_model.fit(X_train, y_train);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nTarget: 0, name: 1\nTop 20 tokens:  ['old', 'good', 'adoption', 'mix', 'amp', 'dogs', 'sweet', 'loves', 'year', 'girl', 'dog', 'meet', 'years', 'female', 'pit', 'pounds', 'application', 'website', 'shelter', 'foster']\n\nTarget: 1, name: 1\nTop 20 tokens:  ['old', 'adoption', 'mix', 'application', 'online', 'website', 'official', 'sweet', 'year', 'meet', 'home', 'looking', 'dog', '039', 'amp', 'forever', 'puppy', 'years', 'dogs', 'loves']\n"
    }
   ],
   "source": [
    "feature_words = count_vect.get_feature_names()\n",
    "n = 20 #number of top words associated with the category that we wish to see\n",
    "\n",
    "for cat in range(len(categories)):\n",
    "    print(f\"\\nTarget: {cat}, name: {target_names[cat]}\")\n",
    "    log_prob = nb_model.feature_log_prob_[cat]\n",
    "    i_topn = np.argsort(log_prob)[::-1][:n]\n",
    "    features_topn = [feature_words[i] for i in i_topn]\n",
    "    print(f\"Top {n} tokens: \", features_topn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = nb_model.predict(X_test)\n",
    "# X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=uint8)"
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.8373983739837398"
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test, y_predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0.73839094, 0.78013778, 0.86613738, 0.91703649, 0.82210813,\n       0.89736222, 0.64970209, 0.97857231, 0.61729211, 0.78835884,\n       0.69789281, 0.80765324, 0.65986287, 0.63361632, 0.74009866,\n       0.60738438, 0.72437236, 0.69844021, 0.73341305, 0.98413922,\n       0.84041125, 0.87084287, 0.69844021, 0.97857231, 0.81512269,\n       0.51829908, 0.68470418, 0.895138  , 0.72883612, 0.79822952,\n       0.63001786, 0.9363682 , 0.92585116, 0.69844021, 0.70106064,\n       0.59799212, 0.69844021, 0.62477217, 0.81231049, 0.81429038,\n       0.80874568, 0.83710784, 0.85924169, 0.97061661, 0.69844021,\n       0.74516362, 0.97857231, 0.8642114 , 0.57937982, 0.76104058,\n       0.67102758, 0.71930099, 0.72168104, 0.79515969, 0.93602649,\n       0.89538821, 0.29806398, 0.65520687, 0.82469423, 0.83594117,\n       0.70994205, 0.66805145, 0.72528225, 0.85373854, 0.77362091,\n       0.77157528, 0.77109597, 0.62500715, 0.84540649, 0.63148413,\n       0.71969035, 0.89529518, 0.89559158, 0.59126199, 0.67477795,\n       0.86562993, 0.81530017, 0.22485339, 0.69844021, 0.80484712,\n       0.69844021, 0.72291191, 0.60606994, 0.69844021, 0.95135623,\n       0.91104295, 0.69844021, 0.88049591, 0.93156875, 0.74131872,\n       0.78066053, 0.78258573, 0.63212577, 0.87197578, 0.7249743 ,\n       0.77501175, 0.91104295, 0.74338462, 0.71989903, 0.77378136,\n       0.85687839, 0.92321387, 0.75224628, 0.97857231, 0.5424284 ,\n       0.95062363, 0.57002901, 0.62238212, 0.88049591, 0.87830106,\n       0.79712019, 0.76803419, 0.59469843, 0.82517663, 0.69844021,\n       0.81653172, 0.69844021, 0.81673577, 0.57531887, 0.7823652 ,\n       0.94445304, 0.94582142, 0.86504417, 0.75248597, 0.86095301,\n       0.82818198, 0.69844021, 0.73164804, 0.76118093, 0.88373456,\n       0.65091533, 0.91825829, 0.72628019, 0.81823377, 0.82517663,\n       0.81262565, 0.55925584, 0.77205496, 0.92508699, 0.65424748,\n       0.65257458, 0.81874952, 0.78066053, 0.67523165, 0.82805782])"
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "source": [
    "y_pred_proba = nb_model.predict_proba(X_test)[:,1]\n",
    "y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n       0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n       0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n       1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,\n       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,\n       1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n       1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1])"
     },
     "metadata": {},
     "execution_count": 88
    }
   ],
   "source": [
    "thresh=0.7\n",
    "y_pred = (y_pred_proba>=thresh).astype(int)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.7783251231527093"
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "source": [
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "# nb_pipeline = Pipeline([('vect', CountVectorizer()),\n",
    "#                         ('tfidf', TfidfTransformer()),\n",
    "#                         ('model', MultinomialNB()),\n",
    "#                         ])\n",
    "# nb_pipeline.fit(twenty_train.data, twenty_train.target);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nThe accuracy on the test set is 0.835.\n"
    }
   ],
   "source": [
    "twenty_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                                     shuffle=True, random_state=42)\n",
    "docs_test = twenty_test.data\n",
    "predicted = nb_pipeline.predict(docs_test)\n",
    "accuracy = np.mean(predicted == twenty_test.target)\n",
    "print(\"\\nThe accuracy on the test set is {0:0.3f}.\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1502"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "len(docs_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}